# Data Validation & Gap Detection Engine

**Component 2** of the Pharma Follow-up Platform - A production-ready validation system for pharmaceutical adverse event reports.

## ğŸ¯ Quick Start

### Run the Pipeline
```bash
cd /Users/shruti/Projects/pharma-followup-platform
source venv/bin/activate
python ai_components/validation/model.py
```

### Launch the Dashboard
```bash
# Method 1: Using shell script
cd ai_components/validation
bash run_dashboard.sh

# Method 2: Direct Streamlit
streamlit run ai_components/validation/dashboard.py
```

Then open **http://localhost:8501** in your browser.

## ğŸ“š Documentation

- **[Dashboard Guide](DASHBOARD_GUIDE.md)** - How to use the interactive dashboard
- **[Engine Summary](../../DATA_VALIDATION_ENGINE_SUMMARY.md)** - Architecture and features
- **[Build Explanation](../../DATA_VALIDATION_STEPWISE_EXPLANATION.md)** - Step-by-step walkthrough
- **[Component Status](../../COMPONENT_2_COMPLETE.md)** - Completion report

## ğŸ“Š What's Included

### 8 Python Modules
- `validation_constants.py` - Configuration and rules
- `data_generator.py` - Synthetic report generation
- `rule_validator.py` - ICH E2B(R3) validation
- `anomaly_detector.py` - Isolation Forest detection
- `completeness_scorer.py` - Field completeness analysis
- `evaluation_metrics.py` - Performance metrics
- `visualizer.py` - Chart generation
- `model.py` - Pipeline orchestration
- `validator.py` - Unified validation system
- `dashboard.py` - Streamlit web interface

### Generated Outputs
- 10,000 validated reports (CSV)
- 7 professional visualizations (PNG @ 300 DPI)
- Detailed metrics (JSON)
- Summary report (TXT)

## ğŸ“ˆ Performance

```
Processing Speed:       320 reports/second
Pipeline Time:          31 seconds for 10,000 reports
Validation Precision:   100% (no false positives)
Validation Recall:      82.5% (catches most errors)
F1-Score:              90.4%
```

## ğŸ” Features

### âœ… Comprehensive Validation
- 14 mandatory field checks
- Categorical value validation
- Numeric range validation
- Date logic verification
- Cross-field consistency checks
- ICH E2B(R3) regulatory compliance

### ğŸ¤– Hybrid Anomaly Detection
- Isolation Forest statistical detection
- Rule-based quality scoring
- 13 engineered features
- Unknown category handling
- Risk classification (Low/Medium/High)

### ğŸ“Š Rich Analytics
- Quality score distribution
- Completeness analysis
- Error detection accuracy
- Anomaly detection metrics
- Confusion matrices
- Error type analysis

### ğŸ¨ Interactive Dashboard
- 5 pages of analysis
- Interactive charts with Plotly
- Data filtering and export
- Real-time metrics
- Professional visualizations

## ğŸ“‹ File Structure

```
ai_components/validation/
â”œâ”€â”€ __init__.py                    # Module initialization
â”œâ”€â”€ validation_constants.py        # Configuration & rules
â”œâ”€â”€ data_generator.py              # Test data creation
â”œâ”€â”€ rule_validator.py              # Validation logic
â”œâ”€â”€ anomaly_detector.py            # ML detection
â”œâ”€â”€ completeness_scorer.py         # Coverage scoring
â”œâ”€â”€ evaluation_metrics.py          # Performance metrics
â”œâ”€â”€ visualizer.py                  # Chart generation
â”œâ”€â”€ validator.py                   # Unified validator
â”œâ”€â”€ model.py                       # Pipeline (main)
â”œâ”€â”€ dashboard.py                   # Streamlit interface
â”œâ”€â”€ run_dashboard.sh              # Dashboard launcher
â””â”€â”€ DASHBOARD_GUIDE.md            # Dashboard documentation

data/processed/
â””â”€â”€ validation_results.csv         # 10,000 reports

evaluation/
â”œâ”€â”€ validation_metrics.json        # Detailed metrics
â”œâ”€â”€ VALIDATION_ENGINE_REPORT.txt  # Summary report
â””â”€â”€ validation_visualizations/
    â”œâ”€â”€ 01_error_distribution.png
    â”œâ”€â”€ 02_quality_score_distribution.png
    â”œâ”€â”€ 03_anomaly_distribution.png
    â”œâ”€â”€ 04_overall_status_distribution.png
    â”œâ”€â”€ 05_quality_vs_anomaly.png
    â”œâ”€â”€ 06_error_types.png
    â””â”€â”€ 07_metrics_summary.png
```

## ğŸš€ Usage Examples

### Generate Validation Data
```python
from validation.data_generator import ValidationDataGenerator

gen = ValidationDataGenerator(num_samples=1000, error_rate=0.35)
df = gen.generate_dataset()
df.to_csv('my_reports.csv')
```

### Validate Reports
```python
from validation.rule_validator import BatchValidator

validator = BatchValidator()
results = validator.validate_dataset(df)
print(results.head())
```

### Detect Anomalies
```python
from validation.anomaly_detector import AnomalyDetector

detector = AnomalyDetector()
detector.train(df[df['has_errors'] == 0])
anomalies = detector.predict(df)
```

### Score Completeness
```python
from validation.completeness_scorer import CompletenessScorer

scorer = CompletenessScorer()
scores = scorer.calculate_scores_batch(df)
print(f"Average: {scores.mean():.1f}%")
```

### Calculate Metrics
```python
from evaluation_metrics import generate_evaluation_report

report = generate_evaluation_report(
    validation_results,
    anomaly_results,
    combined_results,
    original_df
)
```

## ğŸ“Š Dashboard Pages

1. **Overview** - Key metrics and status summary
2. **Visualizations** - Gallery of all 7 charts
3. **Analysis** - Detailed breakdowns and filtering
4. **Metrics** - Performance metrics and confusion matrix
5. **Report** - Full text report

## ğŸ”§ Configuration

### Validation Rules
Edit `validation_constants.py`:
- Add/modify mandatory fields
- Update categorical values
- Adjust numeric ranges
- Change field weights

### Anomaly Detection
Edit `anomaly_detector.py`:
- Adjust contamination threshold
- Modify risk thresholds
- Add/remove features
- Change composite scoring weights

### Data Generation
Edit `data_generator.py`:
- Modify error types
- Adjust error rates
- Change field generation logic

## ğŸ“ˆ Dashboard Features

- **Real-time Refresh** - Click refresh to reload data
- **Interactive Charts** - Hover for tooltips, click legend items
- **Data Export** - Download filtered results as CSV
- **Report Download** - Export full report as TXT
- **Multiple Filters** - Filter by error count, status, quality
- **Responsive Design** - Works on desktop and tablet

## ğŸ”— Integration

### Feeds into:
- **Prioritization Engine** - Quality score as input feature
- **Medical NER** - Cleaned validated data
- **Response Prediction** - Validated + prioritized reports
- **Translation Engine** - Validated structures

### Receives from:
- **Raw Reports** - Adverse event data

## âš™ï¸ Requirements

- Python 3.9+
- Virtual environment
- Dependencies: pandas, numpy, scikit-learn, matplotlib, seaborn, plotly, streamlit, joblib

### Install Dependencies
```bash
pip install -r ../../requirements.txt
```

## ğŸ“ Validation Rules

### Mandatory Fields (14)
- patient_id, patient_age, patient_gender
- event_date, start_date, report_date
- drug_name, dose, dose_unit, route
- event_type, event_description
- outcome, reporter_type

### Validation Checks (6)
1. Mandatory fields present âœ…
2. Correct data types âœ…
3. Categorical values valid âœ…
4. Numeric values in range âœ…
5. Dates logically ordered âœ…
6. Cross-field logic consistent âœ…

### Quality Interpretation
- **Excellent** (80-100%): All critical fields
- **Good** (60-80%): Most important fields
- **Fair** (40-60%): Some fields missing
- **Poor** (20-40%): Many fields missing
- **Critical** (<20%): Most fields missing

## ğŸ“Š Metrics Explained

| Metric | Meaning |
|--------|---------|
| **Precision** | Of errors flagged, how many are real? |
| **Recall** | Of real errors, how many did we catch? |
| **F1-Score** | Balance between precision and recall |
| **FPR** | Of clean data, how many false alarms? |
| **AUC-ROC** | Anomaly detection discrimination power |

## ğŸ› Troubleshooting

### Dashboard Won't Start
```bash
# Check dependencies
pip install streamlit plotly pillow

# Verify data exists
python ai_components/validation/model.py
```

### No Data Found
```bash
# Run pipeline first
python ai_components/validation/model.py
```

### Port Already in Use
```bash
streamlit run dashboard.py --server.port 8502
```

## ğŸ“š Additional Resources

- [Streamlit Documentation](https://docs.streamlit.io)
- [Plotly Documentation](https://plotly.com/python)
- [scikit-learn Isolation Forest](https://scikit-learn.org/stable/modules/ensemble.html#isolation-forest)
- [ICH E2B(R3) Standard](https://www.ich.org)

## ğŸ“ Learning Path

1. Read [Build Explanation](../../DATA_VALIDATION_STEPWISE_EXPLANATION.md)
2. Review [Engine Summary](../../DATA_VALIDATION_ENGINE_SUMMARY.md)
3. Run the pipeline: `python model.py`
4. Launch dashboard: `bash run_dashboard.sh`
5. Explore each dashboard page
6. Read [Dashboard Guide](DASHBOARD_GUIDE.md)
7. Try modifying code and re-running

## ğŸ“… Status

âœ… **PRODUCTION READY**

- All 8 components implemented
- 2,130 lines of production code
- Comprehensive testing completed
- Full documentation provided
- Git history tracked

## ğŸ¯ Next Steps

- Build Component 3: Medical NER
- Build Component 4: Response Prediction
- Build Component 5: Translation Engine
- Create unified platform dashboard
- Deploy to production

## ğŸ“ Support

For issues or questions:
1. Check documentation files
2. Review code comments
3. Test with sample data
4. Check git history for changes

---

**Version**: 1.0  
**Last Updated**: January 7, 2026  
**Status**: âœ… Production Ready  
**Component**: 2 of 5
